{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents.envs import UnityEnvironment\n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents.envs:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of Training Brains : 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaLearning\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space size (per agent): 53\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): [3, 3, 3, 2]\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "train_mode = True  # Whether to run the environment in training or inference mode\n",
    "env_name = \"Tennis_2_agents\"\n",
    "#env_name = \"Banana_2_agents\"\n",
    "env = UnityEnvironment(file_name=env_name)\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import os\n",
    "from collections import deque\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#setting plotting options\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "np.set_printoptions(precision=3, linewidth=120)\n",
    "\n",
    "#hiding matplotlib depreciate warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#creating high resolution outputs\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###path to save the models and collect tensorboard logs\n",
    "model_dir = os.getcwd() + \"/model_dir\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 agents. Each observes a state with length: 53 and act within an action space of length: 4\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "###Instantiating the Unity Env and the Agent\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "num_agents = len(env_info.agents)\n",
    "ENV_ACTION_SIZE = len(brain.vector_action_space_size)\n",
    "#ENV_ACTION_SIZE = len(brain.vector_action_space_size)\n",
    "states = env_info.vector_observations\n",
    "ENV_STATE_SIZE = states.shape[1]\n",
    "\n",
    "print(\"num agents is: \" + str(num_agents) +\"  actions size: \" + str(ENV_ACTION_SIZE) + \" env state size is: \" + str(ENV_STATE_SIZE))\n",
    "'''\n",
    "\n",
    "# Reset the environment    \n",
    "env_info = env.reset(train_mode=True)[brain_name]     \n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "\n",
    "# size of each action\n",
    "ENV_ACTION_SIZE = len(brain.vector_action_space_size)\n",
    "\n",
    "# size of the state space \n",
    "states = env_info.vector_observations  # Array of states for all agents in teh enviroonments\n",
    "ENV_STATE_SIZE = states.shape[1]\n",
    "\n",
    "print('There are {} agents. Each observes a state with length: {} and act within an action space of length: {}'.format(states.shape[0], \n",
    "                                                                                                                       ENV_STATE_SIZE, \n",
    "                                                                                                                       ENV_ACTION_SIZE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 1: 0.0\n",
      "Score (max over agents) from episode 2: 0.0\n",
      "Score (max over agents) from episode 3: 0.0\n",
      "Score (max over agents) from episode 4: 0.0\n",
      "Score (max over agents) from episode 5: 0.0\n"
     ]
    }
   ],
   "source": [
    "def random_agents():    \n",
    "    for i_episode in range(1, 6):                              # play game for 5 episodes\n",
    "        env_info = env.reset(train_mode=False)[brain_name]     # reset the environment\n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        t=0                                                    # Initialize a counter for the nb of steps performed\n",
    "\n",
    "        while True:\n",
    "            t+=1\n",
    "            actions = np.random.randn(num_agents, ENV_ACTION_SIZE) # select an action (for each agent)\n",
    "            actions = np.clip(actions, -1, 1)                      # all actions between -1 and 1         \n",
    "            env_info = env.step(actions)[brain_name]               # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations             # get next state (for each agent)\n",
    "            rewards = env_info.rewards                             # get reward (for each agent)\n",
    "            dones = env_info.local_done                            # see if episode finished\n",
    "\n",
    "            scores += env_info.rewards                         # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                #print(\"   ** Debug: episode= {} steps={} rewards={} dones={}\".format(i_episode, t,rewards,dones))\n",
    "                break\n",
    "        print('Score (max over agents) from episode {}: {}'.format(i_episode, np.max(scores)))\n",
    "\n",
    "random_agents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##MADDPG TRAINING OF AGENT\n",
    "def plot_training(scores):\n",
    "    # Plot the Score evolution during the training\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.tick_params(axis='x', colors='deepskyblue')\n",
    "    ax.tick_params(axis='y', colors='deepskyblue')\n",
    "    plt.plot(np.arange(1, len(scores)+1), scores, color='deepskyblue')\n",
    "    plt.ylabel('Score', color='deepskyblue')\n",
    "    plt.xlabel('Episode #', color='deepskyblue')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MADDPG import MADDPG\n",
    "from hyperparams import *\n",
    "\n",
    "def train():\n",
    "    \n",
    "    # Seeding\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    # Instantiate the MADDPG agents\n",
    "    maddpg = MADDPG(ENV_STATE_SIZE, ENV_ACTION_SIZE, num_agents, SEED)\n",
    "\n",
    "    # Monitor the score    \n",
    "    scores_deque = deque(maxlen=100)\n",
    "    all_scores = []\n",
    "    all_avg_score = []\n",
    "    \n",
    "    \n",
    "    # Intialize amplitude OU noise (will decay during training); done to ensure exploration\n",
    "    noise = NOISE\n",
    "    \n",
    "    all_steps =0   \n",
    "    \n",
    "    # Training Loop\n",
    "    for i_episode in range(NB_EPISODES+1):  \n",
    "        env_info = env.reset(train_mode=True)[brain_name]          \n",
    "        maddpg.reset()                                             \n",
    "        states = env_info.vector_observations                      \n",
    "        scores = np.zeros(num_agents)                              \n",
    "        for steps in range(NB_STEPS):\n",
    "            all_steps+=1\n",
    "            actions = maddpg.act(states, noise)                    \n",
    "            noise *= NOISE_REDUCTION                               \n",
    "            env_info = env.step(actions)[brain_name]               \n",
    "            next_states = env_info.vector_observations             \n",
    "            rewards = env_info.rewards                             \n",
    "            dones = env_info.local_done                            \n",
    "            maddpg.step(states, actions, rewards, next_states, dones, i_episode)  \n",
    "            scores += env_info.rewards                             \n",
    "            #print(\"in episode score is: \" + str(scores))\n",
    "            states = next_states                                   \n",
    "            if np.any(dones):                                      \n",
    "                break\n",
    "        \n",
    "        # Save scores and compute average score over last 100 episodes       \n",
    "        episode_score  = np.max(scores)  \n",
    "        all_scores.append(episode_score)\n",
    "        scores_deque.append(episode_score)\n",
    "        avg_score = np.mean(scores_deque)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tEpisode score (max over agents): {:.2f}'.format(i_episode, avg_score, episode_score), end=\"\")\n",
    "        if i_episode>0 and i_episode % 50 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f} (nb of total steps={}   noise={:.4f})'.format(i_episode, avg_score, all_steps, noise))\n",
    "            maddpg.checkpoints()\n",
    "            all_avg_score.append(avg_score)\n",
    "        # stop if objective is achieved\n",
    "        if (i_episode > 99) and (avg_score >=0.5):\n",
    "            print('\\rEnvironment solved in {} episodes with an Average Score of {:.2f}'.format(i_episode, avg_score))\n",
    "            maddpg.checkpoints()\n",
    "            return all_scores\n",
    "\n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50\tAverage Score: 0.01 (nb of total steps=890   noise=1.0000)\n",
      "Episode 100\tAverage Score: 0.02 (nb of total steps=1773   noise=1.0000)\n",
      "Episode 150\tAverage Score: 0.02 (nb of total steps=2637   noise=1.0000)\n",
      "Episode 200\tAverage Score: 0.02 (nb of total steps=3616   noise=1.0000)\n",
      "Episode 250\tAverage Score: 0.02 (nb of total steps=4472   noise=1.0000)\n",
      "Episode 300\tAverage Score: 0.02 (nb of total steps=5412   noise=1.0000)\n",
      "Episode 350\tAverage Score: 0.03 (nb of total steps=6452   noise=1.0000)\n",
      "Episode 400\tAverage Score: 0.04 (nb of total steps=7438   noise=1.0000)\n",
      "Episode 450\tAverage Score: 0.02 (nb of total steps=8275   noise=1.0000)\n",
      "Episode 500\tAverage Score: 0.01 (nb of total steps=8990   noise=1.0000)\n",
      "Episode 550\tAverage Score: 0.00 (nb of total steps=9738   noise=1.0000)\n",
      "Episode 600\tAverage Score: 0.02 (nb of total steps=10853   noise=1.0000)\n",
      "Episode 650\tAverage Score: 0.04 (nb of total steps=12154   noise=1.0000)\n",
      "Episode 700\tAverage Score: 0.05 (nb of total steps=13486   noise=1.0000)\n",
      "Episode 750\tAverage Score: 0.07 (nb of total steps=15236   noise=1.0000)\n",
      "Episode 800\tAverage Score: 0.08 (nb of total steps=17007   noise=1.0000)\n",
      "Episode 850\tAverage Score: 0.09 (nb of total steps=19093   noise=1.0000)\n",
      "Episode 900\tAverage Score: 0.09 (nb of total steps=20888   noise=1.0000)\n",
      "Episode 950\tAverage Score: 0.09 (nb of total steps=22660   noise=1.0000)\n",
      "Episode 1000\tAverage Score: 0.09 (nb of total steps=24406   noise=1.0000)\n",
      "Episode 1050\tAverage Score: 0.09 (nb of total steps=26126   noise=1.0000)\n",
      "Episode 1100\tAverage Score: 0.09 (nb of total steps=27974   noise=1.0000)\n",
      "Episode 1150\tAverage Score: 0.10 (nb of total steps=29758   noise=1.0000)\n",
      "Episode 1200\tAverage Score: 0.10 (nb of total steps=31914   noise=1.0000)\n",
      "Episode 1250\tAverage Score: 0.10 (nb of total steps=33968   noise=1.0000)\n",
      "Episode 1300\tAverage Score: 0.11 (nb of total steps=36347   noise=1.0000)\n",
      "Episode 1350\tAverage Score: 0.13 (nb of total steps=39283   noise=1.0000)\n",
      "Episode 1400\tAverage Score: 0.13 (nb of total steps=41839   noise=1.0000)\n",
      "Episode 1450\tAverage Score: 0.12 (nb of total steps=44367   noise=1.0000)\n",
      "Episode 1500\tAverage Score: 0.12 (nb of total steps=47090   noise=1.0000)\n",
      "Episode 1550\tAverage Score: 0.13 (nb of total steps=49864   noise=1.0000)\n",
      "Episode 1600\tAverage Score: 0.15 (nb of total steps=53234   noise=1.0000)\n",
      "Episode 1650\tAverage Score: 0.14 (nb of total steps=55743   noise=1.0000)\n",
      "Episode 1700\tAverage Score: 0.15 (nb of total steps=59324   noise=1.0000)\n",
      "Episode 1750\tAverage Score: 0.16 (nb of total steps=62516   noise=1.0000)\n",
      "Episode 1800\tAverage Score: 0.14 (nb of total steps=65490   noise=1.0000)\n",
      "Episode 1850\tAverage Score: 0.13 (nb of total steps=68250   noise=1.0000)\n",
      "Episode 1900\tAverage Score: 0.15 (nb of total steps=72197   noise=1.0000)\n",
      "Episode 1950\tAverage Score: 0.18 (nb of total steps=75942   noise=1.0000)\n",
      "Episode 2000\tAverage Score: 0.17 (nb of total steps=79293   noise=1.0000)\n",
      "Episode 2050\tAverage Score: 0.19 (nb of total steps=83630   noise=1.0000)\n",
      "Episode 2100\tAverage Score: 0.21 (nb of total steps=87773   noise=1.0000)\n",
      "Episode 2150\tAverage Score: 0.21 (nb of total steps=91988   noise=1.0000)\n",
      "Episode 2200\tAverage Score: 0.23 (nb of total steps=96940   noise=1.0000)\n",
      "Episode 2250\tAverage Score: 0.25 (nb of total steps=102018   noise=1.0000)\n",
      "Episode 2300\tAverage Score: 0.27 (nb of total steps=107800   noise=1.0000)\n",
      "Episode 2350\tAverage Score: 0.30 (nb of total steps=114208   noise=1.0000)\n",
      "Episode 2400\tAverage Score: 0.37 (nb of total steps=122550   noise=1.0000)\n",
      "Episode 2450\tAverage Score: 0.36 (nb of total steps=128579   noise=1.0000)\n",
      "Episode 2500\tAverage Score: 0.26 (nb of total steps=133084   noise=1.0000)\n",
      "Episode 2550\tAverage Score: 0.22 (nb of total steps=137260   noise=1.0000)\n",
      "Episode 2600\tAverage Score: 0.21 (nb of total steps=141405   noise=1.0000)\n",
      "Episode 2650\tAverage Score: 0.24 (nb of total steps=146911   noise=1.0000)\n",
      "Episode 2700\tAverage Score: 0.26 (nb of total steps=151858   noise=1.0000)\n",
      "Episode 2750\tAverage Score: 0.27 (nb of total steps=157868   noise=1.0000)\n",
      "Episode 2800\tAverage Score: 0.32 (nb of total steps=164467   noise=1.0000)\n",
      "Episode 2850\tAverage Score: 0.28 (nb of total steps=168967   noise=1.0000)\n",
      "Episode 2900\tAverage Score: 0.22 (nb of total steps=173238   noise=1.0000)\n",
      "Episode 2950\tAverage Score: 0.23 (nb of total steps=178412   noise=1.0000)\n",
      "Episode 3000\tAverage Score: 0.31 (nb of total steps=185509   noise=1.0000)\n",
      "Episode 3050\tAverage Score: 0.33 (nb of total steps=191315   noise=1.0000)\n",
      "Episode 3100\tAverage Score: 0.28 (nb of total steps=196731   noise=1.0000)\n",
      "Episode 3150\tAverage Score: 0.23 (nb of total steps=200628   noise=1.0000)\n",
      "Episode 3200\tAverage Score: 0.20 (nb of total steps=204767   noise=1.0000)\n",
      "Episode 3250\tAverage Score: 0.20 (nb of total steps=208837   noise=1.0000)\n",
      "Episode 3300\tAverage Score: 0.20 (nb of total steps=213068   noise=1.0000)\n",
      "Episode 3350\tAverage Score: 0.21 (nb of total steps=217492   noise=1.0000)\n",
      "Episode 3400\tAverage Score: 0.26 (nb of total steps=223259   noise=1.0000)\n",
      "Episode 3450\tAverage Score: 0.29 (nb of total steps=228801   noise=1.0000)\n",
      "Episode 3500\tAverage Score: 0.30 (nb of total steps=235012   noise=1.0000)\n",
      "Episode 3550\tAverage Score: 0.27 (nb of total steps=239427   noise=1.0000)\n",
      "Episode 3600\tAverage Score: 0.22 (nb of total steps=243767   noise=1.0000)\n",
      "Episode 3650\tAverage Score: 0.25 (nb of total steps=249462   noise=1.0000)\n",
      "Episode 3700\tAverage Score: 0.31 (nb of total steps=256088   noise=1.0000)\n",
      "Episode 3750\tAverage Score: 0.31 (nb of total steps=261546   noise=1.0000)\n",
      "Episode 3800\tAverage Score: 0.32 (nb of total steps=268930   noise=1.0000)\n",
      "Episode 3850\tAverage Score: 0.37 (nb of total steps=276318   noise=1.0000)\n",
      "Episode 3900\tAverage Score: 0.40 (nb of total steps=284844   noise=1.0000)\n",
      "Episode 3950\tAverage Score: 0.43 (nb of total steps=293381   noise=1.0000)\n",
      "Episode 4000\tAverage Score: 0.39 (nb of total steps=300114   noise=1.0000)\n",
      "Episode 4050\tAverage Score: 0.39 (nb of total steps=308528   noise=1.0000)\n",
      "Episode 4100\tAverage Score: 0.40 (nb of total steps=315680   noise=1.0000)\n",
      "Episode 4150\tAverage Score: 0.41 (nb of total steps=324313   noise=1.0000)\n",
      "Episode 4200\tAverage Score: 0.46 (nb of total steps=333543   noise=1.0000)\n",
      "Episode 4250\tAverage Score: 0.44 (nb of total steps=341334   noise=1.0000)\n",
      "Episode 4300\tAverage Score: 0.41 (nb of total steps=349386   noise=1.0000)\n",
      "Episode 4350\tAverage Score: 0.42 (nb of total steps=357456   noise=1.0000)\n",
      "Episode 4400\tAverage Score: 0.38 (nb of total steps=364411   noise=1.0000)\n",
      "Episode 4450\tAverage Score: 0.34 (nb of total steps=370778   noise=1.0000)\n",
      "Episode 4500\tAverage Score: 0.31 (nb of total steps=376845   noise=1.0000)\n",
      "Episode 4550\tAverage Score: 0.31 (nb of total steps=383269   noise=1.0000)\n",
      "Episode 4600\tAverage Score: 0.27 (nb of total steps=387424   noise=1.0000)\n",
      "Episode 4650\tAverage Score: 0.24 (nb of total steps=392631   noise=1.0000)\n",
      "Episode 4700\tAverage Score: 0.26 (nb of total steps=397627   noise=1.0000)\n",
      "Episode 4750\tAverage Score: 0.25 (nb of total steps=402838   noise=1.0000)\n",
      "Episode 4800\tAverage Score: 0.26 (nb of total steps=408188   noise=1.0000)\n",
      "Episode 4850\tAverage Score: 0.24 (nb of total steps=412658   noise=1.0000)\n",
      "Episode 4900\tAverage Score: 0.23 (nb of total steps=417378   noise=1.0000)\n",
      "Episode 4950\tAverage Score: 0.24 (nb of total steps=422408   noise=1.0000)\n",
      "Episode 5000\tAverage Score: 0.24 (nb of total steps=427245   noise=1.0000)\n",
      "Episode 5050\tAverage Score: 0.28 (nb of total steps=433388   noise=1.0000)\n",
      "Episode 5100\tAverage Score: 0.25 (nb of total steps=437393   noise=1.0000)\n",
      "Episode 5150\tAverage Score: 0.19 (nb of total steps=441320   noise=1.0000)\n",
      "Episode 5200\tAverage Score: 0.19 (nb of total steps=445175   noise=1.0000)\n",
      "Episode 5250\tAverage Score: 0.24 (nb of total steps=451155   noise=1.0000)\n",
      "Episode 5300\tAverage Score: 0.27 (nb of total steps=455984   noise=1.0000)\n",
      "Episode 5350\tAverage Score: 0.27 (nb of total steps=461759   noise=1.0000)\n",
      "Episode 5400\tAverage Score: 0.26 (nb of total steps=466583   noise=1.0000)\n",
      "Episode 5450\tAverage Score: 0.23 (nb of total steps=471465   noise=1.0000)\n",
      "Episode 5500\tAverage Score: 0.23 (nb of total steps=476141   noise=1.0000)\n",
      "Episode 5550\tAverage Score: 0.25 (nb of total steps=481975   noise=1.0000)\n",
      "Episode 5600\tAverage Score: 0.26 (nb of total steps=486928   noise=1.0000)\n",
      "Episode 5650\tAverage Score: 0.26 (nb of total steps=492723   noise=1.0000)\n",
      "Episode 5700\tAverage Score: 0.23 (nb of total steps=496298   noise=1.0000)\n",
      "Episode 5750\tAverage Score: 0.17 (nb of total steps=499923   noise=1.0000)\n",
      "Episode 5800\tAverage Score: 0.19 (nb of total steps=504055   noise=1.0000)\n",
      "Episode 5850\tAverage Score: 0.19 (nb of total steps=507670   noise=1.0000)\n",
      "Episode 5900\tAverage Score: 0.17 (nb of total steps=511099   noise=1.0000)\n",
      "Episode 5950\tAverage Score: 0.17 (nb of total steps=514813   noise=1.0000)\n",
      "Episode 6000\tAverage Score: 0.16 (nb of total steps=518279   noise=1.0000)\n",
      "Episode 6050\tAverage Score: 0.18 (nb of total steps=522377   noise=1.0000)\n",
      "Episode 6100\tAverage Score: 0.21 (nb of total steps=526993   noise=1.0000)\n",
      "Episode 6150\tAverage Score: 0.19 (nb of total steps=530448   noise=1.0000)\n",
      "Episode 6200\tAverage Score: 0.16 (nb of total steps=533801   noise=1.0000)\n",
      "Episode 6250\tAverage Score: 0.18 (nb of total steps=538046   noise=1.0000)\n",
      "Episode 6300\tAverage Score: 0.17 (nb of total steps=541272   noise=1.0000)\n",
      "Episode 6350\tAverage Score: 0.16 (nb of total steps=544908   noise=1.0000)\n",
      "Episode 6400\tAverage Score: 0.16 (nb of total steps=548061   noise=1.0000)\n",
      "Episode 6450\tAverage Score: 0.12 (nb of total steps=550570   noise=1.0000)\n",
      "Episode 6500\tAverage Score: 0.13 (nb of total steps=553858   noise=1.0000)\n",
      "Episode 6550\tAverage Score: 0.18 (nb of total steps=558419   noise=1.0000)\n",
      "Episode 6600\tAverage Score: 0.21 (nb of total steps=562623   noise=1.0000)\n",
      "Episode 6650\tAverage Score: 0.20 (nb of total steps=566548   noise=1.0000)\n",
      "Episode 6700\tAverage Score: 0.20 (nb of total steps=570683   noise=1.0000)\n",
      "Episode 6750\tAverage Score: 0.18 (nb of total steps=574002   noise=1.0000)\n",
      "Episode 6800\tAverage Score: 0.15 (nb of total steps=576971   noise=1.0000)\n",
      "Episode 6850\tAverage Score: 0.13 (nb of total steps=579558   noise=1.0000)\n",
      "Episode 6900\tAverage Score: 0.11 (nb of total steps=581912   noise=1.0000)\n",
      "Episode 6950\tAverage Score: 0.14 (nb of total steps=585414   noise=1.0000)\n",
      "Episode 7000\tAverage Score: 0.16 (nb of total steps=588688   noise=1.0000)\n",
      "Episode 7050\tAverage Score: 0.17 (nb of total steps=592796   noise=1.0000)\n",
      "Episode 7100\tAverage Score: 0.21 (nb of total steps=597530   noise=1.0000)\n",
      "Episode 7150\tAverage Score: 0.19 (nb of total steps=600577   noise=1.0000)\n",
      "Episode 7200\tAverage Score: 0.16 (nb of total steps=604048   noise=1.0000)\n",
      "Episode 7250\tAverage Score: 0.16 (nb of total steps=607527   noise=1.0000)\n",
      "Episode 7300\tAverage Score: 0.15 (nb of total steps=610542   noise=1.0000)\n",
      "Episode 7350\tAverage Score: 0.15 (nb of total steps=614094   noise=1.0000)\n",
      "Episode 7400\tAverage Score: 0.15 (nb of total steps=617202   noise=1.0000)\n",
      "Episode 7427\tAverage Score: 0.13\tEpisode score (max over agents): 0.09"
     ]
    }
   ],
   "source": [
    "scores = train()\n",
    "plot_training(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
