{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents.envs import UnityEnvironment\n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents.envs:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of Training Brains : 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaLearning\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space size (per agent): 53\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): [3, 3, 3, 2]\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "train_mode = True  # Whether to run the environment in training or inference mode\n",
    "#env_name = \"Tennis_2_agents\"\n",
    "env_name = \"Banana_2_agents\"\n",
    "env = UnityEnvironment(file_name=env_name, worker_id=1, seed=1)\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import os\n",
    "from collections import deque\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#setting plotting options\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "np.set_printoptions(precision=3, linewidth=120)\n",
    "\n",
    "#hiding matplotlib depreciate warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#creating high resolution outputs\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###path to save the models and collect tensorboard logs\n",
    "model_dir = os.getcwd() + \"/model_dir\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 agents. Each observes a state with length: 53 and act within an action space of length: 4\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "###Instantiating the Unity Env and the Agent\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "num_agents = len(env_info.agents)\n",
    "ENV_ACTION_SIZE = len(brain.vector_action_space_size)\n",
    "#ENV_ACTION_SIZE = len(brain.vector_action_space_size)\n",
    "states = env_info.vector_observations\n",
    "ENV_STATE_SIZE = states.shape[1]\n",
    "\n",
    "print(\"num agents is: \" + str(num_agents) +\"  actions size: \" + str(ENV_ACTION_SIZE) + \" env state size is: \" + str(ENV_STATE_SIZE))\n",
    "'''\n",
    "\n",
    "# Reset the environment    \n",
    "env_info = env.reset(train_mode=True)[brain_name]     \n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "\n",
    "# size of each action\n",
    "ENV_ACTION_SIZE = len(brain.vector_action_space_size)\n",
    "\n",
    "# size of the state space \n",
    "states = env_info.vector_observations  # Array of states for all agents in teh enviroonments\n",
    "ENV_STATE_SIZE = states.shape[1]\n",
    "\n",
    "print('There are {} agents. Each observes a state with length: {} and act within an action space of length: {}'.format(states.shape[0], \n",
    "                                                                                                                       ENV_STATE_SIZE, \n",
    "                                                                                                                       ENV_ACTION_SIZE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 1: 0.0\n",
      "Score (max over agents) from episode 2: 0.0\n",
      "Score (max over agents) from episode 3: 0.0\n",
      "Score (max over agents) from episode 4: 0.0\n",
      "Score (max over agents) from episode 5: 0.0\n"
     ]
    }
   ],
   "source": [
    "def random_agents():    \n",
    "    for i_episode in range(1, 6):                              # play game for 5 episodes\n",
    "        env_info = env.reset(train_mode=False)[brain_name]     # reset the environment\n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        t=0                                                    # Initialize a counter for the nb of steps performed\n",
    "\n",
    "        while True:\n",
    "            t+=1\n",
    "            actions = np.random.randn(num_agents, ENV_ACTION_SIZE) # select an action (for each agent)\n",
    "            actions = np.clip(actions, -1, 1)                      # all actions between -1 and 1         \n",
    "            env_info = env.step(actions)[brain_name]               # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations             # get next state (for each agent)\n",
    "            rewards = env_info.rewards                             # get reward (for each agent)\n",
    "            dones = env_info.local_done                            # see if episode finished\n",
    "\n",
    "            scores += env_info.rewards                         # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                #print(\"   ** Debug: episode= {} steps={} rewards={} dones={}\".format(i_episode, t,rewards,dones))\n",
    "                break\n",
    "        print('Score (max over agents) from episode {}: {}'.format(i_episode, np.max(scores)))\n",
    "\n",
    "random_agents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##MADDPG TRAINING OF AGENT\n",
    "def plot_training(scores):\n",
    "    # Plot the Score evolution during the training\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.tick_params(axis='x', colors='deepskyblue')\n",
    "    ax.tick_params(axis='y', colors='deepskyblue')\n",
    "    plt.plot(np.arange(1, len(scores)+1), scores, color='deepskyblue')\n",
    "    plt.ylabel('Score', color='deepskyblue')\n",
    "    plt.xlabel('Episode #', color='deepskyblue')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MADDPG import MADDPG\n",
    "from hyperparams import *\n",
    "\n",
    "def train():\n",
    "    \n",
    "    # Seeding\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    # Instantiate the MADDPG agents\n",
    "    maddpg = MADDPG(ENV_STATE_SIZE, ENV_ACTION_SIZE, num_agents, SEED)\n",
    "\n",
    "    # Monitor the score    \n",
    "    scores_deque = deque(maxlen=100)\n",
    "    all_scores = []\n",
    "    all_avg_score = []\n",
    "    \n",
    "    \n",
    "    # Intialize amplitude OU noise (will decay during training); done to ensure exploration\n",
    "    noise = NOISE\n",
    "    \n",
    "    all_steps =0   \n",
    "    \n",
    "    # Training Loop\n",
    "    for i_episode in range(NB_EPISODES+1):  \n",
    "        env_info = env.reset(train_mode=True)[brain_name]          \n",
    "        maddpg.reset()                                             \n",
    "        states = env_info.vector_observations                      \n",
    "        scores = np.zeros(num_agents)                              \n",
    "        for steps in range(NB_STEPS):\n",
    "            all_steps+=1\n",
    "            actions = maddpg.act(states, noise)                    \n",
    "            noise *= NOISE_REDUCTION                               \n",
    "            env_info = env.step(actions)[brain_name]               \n",
    "            next_states = env_info.vector_observations             \n",
    "            rewards = env_info.rewards                             \n",
    "            dones = env_info.local_done                            \n",
    "            maddpg.step(states, actions, rewards, next_states, dones, i_episode)  \n",
    "            scores += env_info.rewards                             \n",
    "            #print(\"in episode score is: \" + str(scores))\n",
    "            states = next_states                                   \n",
    "            if np.any(dones):                                      \n",
    "                break\n",
    "        \n",
    "        # Save scores and compute average score over last 100 episodes       \n",
    "        episode_score  = np.max(scores)  \n",
    "        all_scores.append(episode_score)\n",
    "        scores_deque.append(episode_score)\n",
    "        avg_score = np.mean(scores_deque)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tEpisode score (max over agents): {:.2f}'.format(i_episode, avg_score, episode_score), end=\"\")\n",
    "        if i_episode>0 and i_episode % 50 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f} (nb of total steps={}   noise={:.4f})'.format(i_episode, avg_score, all_steps, noise))\n",
    "            maddpg.checkpoints()\n",
    "            all_avg_score.append(avg_score)\n",
    "        # stop if objective is achieved\n",
    "        if (i_episode > 99) and (avg_score >=0.5):\n",
    "            print('\\rEnvironment solved in {} episodes with an Average Score of {:.2f}'.format(i_episode, avg_score))\n",
    "            maddpg.checkpoints()\n",
    "            return all_scores\n",
    "\n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 15\tAverage Score: 0.69\tEpisode score (max over agents): 0.00"
     ]
    }
   ],
   "source": [
    "scores = train()\n",
    "plot_training(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
