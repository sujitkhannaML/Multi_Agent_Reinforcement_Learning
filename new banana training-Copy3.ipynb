{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents.envs import UnityEnvironment\n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents.envs:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of Training Brains : 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaLearning\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space size (per agent): 53\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): [3, 3, 3, 2]\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "train_mode = True  # Whether to run the environment in training or inference mode\n",
    "#env_name = \"Tennis_2_agents\"\n",
    "env_name = \"Banana_2_agents\"\n",
    "env = UnityEnvironment(file_name=env_name, worker_id=1, seed=1)\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import os\n",
    "from collections import deque\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#setting plotting options\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "np.set_printoptions(precision=3, linewidth=120)\n",
    "\n",
    "#hiding matplotlib depreciate warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#creating high resolution outputs\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###path to save the models and collect tensorboard logs\n",
    "model_dir = os.getcwd() + \"/model_dir\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 agents. Each observes a state with length: 53 and act within an action space of length: 4\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "###Instantiating the Unity Env and the Agent\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "num_agents = len(env_info.agents)\n",
    "ENV_ACTION_SIZE = len(brain.vector_action_space_size)\n",
    "#ENV_ACTION_SIZE = len(brain.vector_action_space_size)\n",
    "states = env_info.vector_observations\n",
    "ENV_STATE_SIZE = states.shape[1]\n",
    "\n",
    "print(\"num agents is: \" + str(num_agents) +\"  actions size: \" + str(ENV_ACTION_SIZE) + \" env state size is: \" + str(ENV_STATE_SIZE))\n",
    "'''\n",
    "\n",
    "# Reset the environment    \n",
    "env_info = env.reset(train_mode=True)[brain_name]     \n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "\n",
    "# size of each action\n",
    "ENV_ACTION_SIZE = len(brain.vector_action_space_size)\n",
    "\n",
    "# size of the state space \n",
    "states = env_info.vector_observations  # Array of states for all agents in teh enviroonments\n",
    "ENV_STATE_SIZE = states.shape[1]\n",
    "\n",
    "print('There are {} agents. Each observes a state with length: {} and act within an action space of length: {}'.format(states.shape[0], \n",
    "                                                                                                                       ENV_STATE_SIZE, \n",
    "                                                                                                                       ENV_ACTION_SIZE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 1: 0.0\n",
      "Score (max over agents) from episode 2: 0.0\n",
      "Score (max over agents) from episode 3: 0.0\n",
      "Score (max over agents) from episode 4: 0.0\n",
      "Score (max over agents) from episode 5: 0.0\n"
     ]
    }
   ],
   "source": [
    "def random_agents():    \n",
    "    for i_episode in range(1, 6):                              # play game for 5 episodes\n",
    "        env_info = env.reset(train_mode=False)[brain_name]     # reset the environment\n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        t=0                                                    # Initialize a counter for the nb of steps performed\n",
    "\n",
    "        while True:\n",
    "            t+=1\n",
    "            actions = np.random.randn(num_agents, ENV_ACTION_SIZE) # select an action (for each agent)\n",
    "            actions = np.clip(actions, -1, 1)                      # all actions between -1 and 1         \n",
    "            env_info = env.step(actions)[brain_name]               # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations             # get next state (for each agent)\n",
    "            rewards = env_info.rewards                             # get reward (for each agent)\n",
    "            dones = env_info.local_done                            # see if episode finished\n",
    "\n",
    "            scores += env_info.rewards                         # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                #print(\"   ** Debug: episode= {} steps={} rewards={} dones={}\".format(i_episode, t,rewards,dones))\n",
    "                break\n",
    "        print('Score (max over agents) from episode {}: {}'.format(i_episode, np.max(scores)))\n",
    "\n",
    "random_agents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##MADDPG TRAINING OF AGENT\n",
    "def plot_training(scores):\n",
    "    # Plot the Score evolution during the training\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.tick_params(axis='x', colors='deepskyblue')\n",
    "    ax.tick_params(axis='y', colors='deepskyblue')\n",
    "    plt.plot(np.arange(1, len(scores)+1), scores, color='deepskyblue')\n",
    "    plt.ylabel('Score', color='deepskyblue')\n",
    "    plt.xlabel('Episode #', color='deepskyblue')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MADDPG import MADDPG\n",
    "from hyperparams import *\n",
    "\n",
    "def train():\n",
    "    \n",
    "    # Seeding\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    # Instantiate the MADDPG agents\n",
    "    maddpg = MADDPG(ENV_STATE_SIZE, ENV_ACTION_SIZE, num_agents, SEED)\n",
    "\n",
    "    # Monitor the score    \n",
    "    scores_deque = deque(maxlen=100)\n",
    "    all_scores = []\n",
    "    all_avg_score = []\n",
    "    \n",
    "    \n",
    "    # Intialize amplitude OU noise (will decay during training); done to ensure exploration\n",
    "    noise = NOISE\n",
    "    \n",
    "    all_steps =0   \n",
    "    \n",
    "    # Training Loop\n",
    "    for i_episode in range(NB_EPISODES+1):  \n",
    "        env_info = env.reset(train_mode=True)[brain_name]          \n",
    "        maddpg.reset()                                             \n",
    "        states = env_info.vector_observations                      \n",
    "        scores = np.zeros(num_agents)                              \n",
    "        for steps in range(NB_STEPS):\n",
    "            all_steps+=1\n",
    "            actions = maddpg.act(states, noise)                    \n",
    "            noise *= NOISE_REDUCTION                               \n",
    "            env_info = env.step(actions)[brain_name]               \n",
    "            next_states = env_info.vector_observations             \n",
    "            rewards = env_info.rewards                             \n",
    "            dones = env_info.local_done                            \n",
    "            maddpg.step(states, actions, rewards, next_states, dones, i_episode)  \n",
    "            scores += env_info.rewards                             \n",
    "            #print(\"in episode score is: \" + str(scores))\n",
    "            states = next_states                                   \n",
    "            if np.any(dones):                                      \n",
    "                break\n",
    "        \n",
    "        # Save scores and compute average score over last 100 episodes       \n",
    "        episode_score  = np.max(scores)  \n",
    "        all_scores.append(episode_score)\n",
    "        scores_deque.append(episode_score)\n",
    "        avg_score = np.mean(scores_deque)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tEpisode score (max over agents): {:.2f}'.format(i_episode, avg_score, episode_score), end=\"\")\n",
    "        if i_episode>0 and i_episode % 50 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f} (nb of total steps={}   noise={:.4f})'.format(i_episode, avg_score, all_steps, noise))\n",
    "            maddpg.checkpoints()\n",
    "            all_avg_score.append(avg_score)\n",
    "        # stop if objective is achieved\n",
    "        if (i_episode > 99) and (avg_score >=0.5):\n",
    "            print('\\rEnvironment solved in {} episodes with an Average Score of {:.2f}'.format(i_episode, avg_score))\n",
    "            maddpg.checkpoints()\n",
    "            return all_scores\n",
    "\n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 23\tAverage Score: 0.62\tEpisode score (max over agents): 1.000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a274bd26db2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplot_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-d7dc95d962be>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mmaddpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mscores\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m#print(\"in episode score is: \" + str(scores))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ResearchPapers/Code/Research/Banana_MADDPG/.idea/MADDPG.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, states, actions, rewards, next_states, dones, num_current_episode)\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;34m'''update agent 1'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaddpg_learn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mown_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmaddpg_learn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mown_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ResearchPapers/Code/Research/Banana_MADDPG/.idea/MADDPG.py\u001b[0m in \u001b[0;36mmaddpg_learn\u001b[0;34m(self, experiences, own_idx, other_idx, gamma)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# Minimize the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mcritic_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mCLIP_CRITIC_GRADIENT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = train()\n",
    "plot_training(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
